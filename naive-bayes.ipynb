{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are are going to implement the Naive Bayes from scratch to understand the application of Bayes Theorem by creating a Text Classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Theorem\n",
    "\n",
    "P(Class | Document) = (P(Document | Class) * P(Class))  / P(Document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Class | Document) = Probability that the document belong to a particular class given its content\n",
    "\n",
    "P(Document | Class) = The Likelihood of the document's content given that it belong to the class. This is where the \"naive\" assumption comes in; we assume that the presence of one word in a document is independent of the presence of the other word.\n",
    "\n",
    "P(Class) = The prior probability of the class(how often this class occur in the dataset)\n",
    "\n",
    "P(Document) = The probability of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior Probability: P(Class) Calculated from the frequency of each class in the training set.\n",
    "\n",
    "Likelihood: P(Document | Class) Assuming Independence. This will become the product of all the probabilities of P(Wi | Class)\n",
    "\n",
    "Where Wi is each word in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Wi | Class) = (Count(Wi, Class) + alpha)/(Count(AllWords, class) + alpha * |number of uniqe words|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction: Calculate the probability for each class using the log-sum trick to avoid underflow, since we are multiplying many probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # Alpha is for Laplace smoothing to handle words not seen in training\n",
    "        self.alpha = alpha\n",
    "        self.classes = None\n",
    "        self.vocab = None\n",
    "        self.class_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_counts = defaultdict(int)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Convert to lowercase and split text into words\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize classes and vocabulary\n",
    "        self.classes = np.unique(y)\n",
    "        self.vocab = set()\n",
    "        \n",
    "        # Count words for each class\n",
    "        for text, label in zip(X, y):\n",
    "            words = self.tokenize(text)\n",
    "            self.class_counts[label] += 1  # Count of documents in each class\n",
    "            for word in words:\n",
    "                self.class_word_counts[label][word] += 1  # Count of word in class\n",
    "                self.vocab.add(word)  # Add to vocabulary\n",
    "\n",
    "        # Convert vocab set to list for indexing\n",
    "        self.vocab = list(self.vocab)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for text in X:\n",
    "            words = self.tokenize(text)\n",
    "            class_probs = {}\n",
    "            for c in self.classes:\n",
    "                # Prior probability of class\n",
    "                log_prior = np.log(self.class_counts[c] / sum(self.class_counts.values()))\n",
    "                # Likelihood of text given class\n",
    "                log_likelihood = 0\n",
    "                for word in words:\n",
    "                    word_count = self.class_word_counts[c][word]\n",
    "                    total_words = sum(self.class_word_counts[c].values())\n",
    "                    log_likelihood += np.log((word_count + self.alpha) / (total_words + self.alpha * len(self.vocab)))\n",
    "                # Combine prior and likelihood\n",
    "                class_probs[c] = log_prior + log_likelihood\n",
    "\n",
    "            # Choose class with highest probability\n",
    "            predictions.append(max(class_probs, key=class_probs.get))\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training data\n",
    "X_train = [\n",
    "    \"Get rich quick with our new scheme\",  # Spam\n",
    "    \"Meeting at 2 PM today\",               # Not Spam\n",
    "    \"Win a free iPhone now\",               # Spam\n",
    "    \"Project report due tomorrow\",         # Not Spam\n",
    "    \"Claim your lottery prize\",            # Spam\n",
    "    \"Lunch at the cafeteria at 12\",        # Not Spam\n",
    "]\n",
    "\n",
    "y_train = ['spam', 'not spam', 'spam', 'not spam', 'spam', 'not spam']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.str_('spam'), np.str_('not spam')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
